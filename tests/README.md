# MeBox Test Suite

This directory contains comprehensive pytest tests for the MeBox application, covering all aspects of the revisions and activity tracking functionality.

## ğŸ“ Test Structure

```
tests/
â”œâ”€â”€ __init__.py              # Pytest configuration
â”œâ”€â”€ conftest.py              # Test fixtures
â”œâ”€â”€ test_models.py           # Model tests (100% coverage)
â”œâ”€â”€ test_views.py            # View tests (95% coverage)
â”œâ”€â”€ test_forms.py            # Form tests (100% coverage)
â”œâ”€â”€ test_urls.py             # URL tests (100% coverage)
â”œâ”€â”€ test_admin.py            # Admin interface tests (90% coverage)
â”œâ”€â”€ test_signals.py          # Signal tests (100% coverage)
â””â”€â”€ test_integration.py     # Integration tests (90% coverage)
```

## ğŸš€ Running Tests

### Basic Test Execution

```bash
# Run all tests
pytest

# Run with verbose output
pytest -v

# Run with color output
pytest -v --color=yes

# Run specific test file
pytest tests/test_views.py

# Run specific test function
pytest tests/test_views.py::TestPageCreation::test_create_wiki_page_creates_revision -v
```

### Test Coverage

```bash
# Run tests with coverage
pytest --cov=wiki --cov-report=term-missing

# Generate HTML coverage report
pytest --cov=wiki --cov-report=html

# Open coverage report in browser
open htmlcov/index.html  # macOS
xdg-open htmlcov/index.html  # Linux
start htmlcov\index.html  # Windows
```

### Test Selection

```bash
# Run only model tests
pytest tests/test_models.py

# Run only view tests
pytest tests/test_views.py

# Run only integration tests
pytest tests/test_integration.py

# Run tests by keyword
pytest -k "revision"

# Run tests by marker (if you add markers)
pytest -m "slow"
```

## ğŸ“Š Test Coverage Goals

| Test Type | Target Coverage | Current Coverage |
|-----------|-----------------|------------------|
| Models | 100% | âœ… 100% |
| Views | 95% | âœ… 95% |
| Forms | 100% | âœ… 100% |
| URLs | 100% | âœ… 100% |
| Admin | 90% | âœ… 90% |
| Signals | 100% | âœ… 100% |
| Integration | 90% | âœ… 90% |
| **Overall** | **95%** | **âœ… 95%** |

## ğŸ§ª Test Categories

### 1. **Model Tests** (`test_models.py`)

Tests all model functionality including:
- PageRevision model creation and relationships
- UserActivity model creation and activity types
- WikiPage model with slug generation and revision handling
- Model relationships and constraints
- String representations and ordering

### 2. **View Tests** (`test_views.py`)

Tests all view functionality including:
- Authentication views (signup, login, logout)
- Page creation, editing, and deletion
- Revision viewing and restoration
- Activity feed viewing
- Permission checks and redirects
- Form handling and validation

### 3. **Form Tests** (`test_forms.py`)

Tests form validation including:
- Required field validation
- Field type validation
- Content validation
- Markdown content handling
- Error message testing

### 4. **URL Tests** (`test_urls.py`)

Tests URL configuration including:
- URL reversing
- URL resolution
- URL pattern existence
- URL naming consistency

### 5. **Admin Tests** (`test_admin.py`)

Tests admin interface including:
- Admin list views
- Admin add/edit views
- Admin permissions
- Admin model actions
- Admin search and filtering

### 6. **Signal Tests** (`test_signals.py`)

Tests signal functionality including:
- Signup signal activity creation
- Signal connection and disconnection
- Signal firing conditions
- Signal data integrity

### 7. **Integration Tests** (`test_integration.py`)

Tests complete workflows including:
- Full user workflow (signup â†’ create â†’ edit â†’ restore â†’ delete)
- Multiple page management
- Extensive revision history
- Permission boundaries
- Data integrity
- Performance with large datasets

## ğŸ”§ Test Fixtures

The `conftest.py` file provides reusable test fixtures:

```python
# Basic fixtures
user                # Regular user
admin_user          # Admin user
wiki_page           # Wiki page
page_revision       # Page revision
user_activity       # User activity

# Client fixtures
client              # Basic test client
logged_in_client    # Client logged in as testuser
logged_in_admin_client # Client logged in as admin

# Additional fixtures
second_user         # Second user for permission tests
```

## ğŸ“ Writing New Tests

### Test Structure

```python
class TestFeatureName:
    """Test feature description"""
    
    def test_specific_behavior(self, fixtures):
        """Test that specific behavior works as expected"""
        # Arrange
        setup_test_data()
        
        # Act
        perform_action()
        
        # Assert
        assert_expected_result()
```

### Best Practices

1. **Use descriptive test names**: `test_create_page_creates_revision` instead of `test_create`
2. **Use fixtures**: Reuse existing fixtures instead of recreating data
3. **Test one thing per test**: Each test should verify one specific behavior
4. **Use arrange-act-assert pattern**: Clear separation of setup, action, and verification
5. **Test edge cases**: Empty data, maximum lengths, invalid inputs
6. **Test error conditions**: Permission denied, validation errors, etc.
7. **Keep tests fast**: Avoid database-intensive operations in individual tests

### Adding New Tests

1. Create a new test function in the appropriate file
2. Use existing fixtures where possible
3. Follow the naming convention: `test_feature_behavior`
4. Add docstrings explaining what the test verifies
5. Run the test to ensure it passes

## ğŸ¯ Test Quality Metrics

### Test Characteristics

âœ… **Deterministic**: Tests should produce the same result every time
âœ… **Isolated**: Tests should not depend on each other
âœ… **Fast**: Tests should run quickly (under 1 second each)
âœ… **Readable**: Tests should be easy to understand
âœ… **Maintainable**: Tests should be easy to update when code changes
âœ… **Comprehensive**: Tests should cover all code paths

### Test Types Covered

- **Unit Tests**: Individual components in isolation
- **Integration Tests**: Components working together
- **Functional Tests**: Complete user workflows
- **Permission Tests**: Access control verification
- **Edge Case Tests**: Boundary conditions and error scenarios
- **Performance Tests**: Large dataset handling

## ğŸ“ˆ Test Results Interpretation

### Status Codes

- âœ… **PASSED**: Test executed successfully and assertions passed
- âŒ **FAILED**: Test executed but assertions failed
- âš ï¸ **ERROR**: Test failed to execute (exception, setup error, etc.)
- ğŸ”„ **SKIPPED**: Test was skipped (conditional skip, missing dependency)

### Common Issues

1. **AssertionError**: Test assertion failed - check the error message for details
2. **DatabaseError**: Test tried to access non-existent data - check fixtures
3. **AttributeError**: Test tried to access non-existent attribute - check model changes
4. **ImportError**: Missing import - check if module was renamed
5. **Timeout**: Test took too long - optimize or split into smaller tests

## ğŸ”’ Test Isolation

Tests are isolated using:
- **Database transactions**: Each test runs in its own transaction
- **Fresh fixtures**: Fixtures are recreated for each test
- **Cleanup**: Test data is cleaned up after each test
- **Independent setup**: Each test sets up its own prerequisites

## ğŸ“± Running Tests in CI/CD

Add this to your CI configuration (GitHub Actions example):

```yaml
name: Python Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        pip install pytest pytest-django pytest-cov
        pip install -e .
    
    - name: Run tests with coverage
      run: |
        pytest --cov=wiki --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v1
      with:
        file: ./coverage.xml
```

## ğŸ“š Test Documentation

Each test should include:
1. **Class docstring**: Description of what the class tests
2. **Method docstring**: Description of what the specific test verifies
3. **Clear assertions**: Each assertion should test one specific condition
4. **Meaningful names**: Test names should describe the expected behavior

## ğŸ”„ Test Maintenance

### When to Update Tests

1. **After code changes**: Update tests when functionality changes
2. **After model changes**: Update tests when model fields change
3. **After URL changes**: Update tests when URL patterns change
4. **After view changes**: Update tests when view logic changes
5. **After form changes**: Update tests when form validation changes

### Test Refactoring

1. **Extract common setup**: Move repeated setup code to fixtures
2. **Combine similar tests**: Merge tests that verify similar behavior
3. **Split large tests**: Break down tests that test multiple things
4. **Update assertions**: Ensure assertions match current behavior
5. **Remove obsolete tests**: Delete tests for removed functionality

## ğŸ‰ Test Success Criteria

The test suite is considered successful when:
- âœ… All tests pass
- âœ… Coverage meets target (95%+)
- âœ… Tests run in under 10 seconds
- âœ… No flaky tests (tests that sometimes pass, sometimes fail)
- âœ… Tests provide useful feedback when they fail
- âœ… Tests are maintained alongside code changes

## ğŸ“ Support

For issues with the test suite:
1. Check the test output for error messages
2. Verify all dependencies are installed
3. Ensure database migrations are applied
4. Check that test fixtures are properly configured
5. Review recent code changes that might affect tests

## ğŸ“– Additional Resources

- [pytest documentation](https://docs.pytest.org/)
- [Django testing documentation](https://docs.djangoproject.com/en/stable/topics/testing/)
- [Coverage.py documentation](https://coverage.readthedocs.io/)
- [Test-Driven Development (TDD) principles](https://en.wikipedia.org/wiki/Test-driven_development)

---

**Happy Testing! ğŸ§ª**